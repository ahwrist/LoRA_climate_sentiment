{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f35354cd",
   "metadata": {},
   "source": [
    "# Lightweight Fine-Tuning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560fb3ff",
   "metadata": {},
   "source": [
    "TODO: In this cell, describe your choices for each of the following\n",
    "\n",
    "* PEFT technique: LoRA (Low Rank Adaptation)\n",
    "* Model: \n",
    "* Evaluation approach: \n",
    "* Fine-tuning dataset: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d76bb",
   "metadata": {},
   "source": [
    "## Loading and Evaluating a Foundation Model\n",
    "\n",
    "TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4935cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required versino of datasets if needed (uncomment to run)\n",
    "# You may need to restart the kernel after running this cell\n",
    "# ! pip install -q \"datasets==2.15.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f28c4a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewwrist/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label'],\n",
       "     num_rows: 320\n",
       " })}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Climate Sentiment dataset from Hugging Face\n",
    "# Link for more info: https://huggingface.co/datasets/climatebert/climate_sentiment?row=12\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the train and test splits of the climate_sentiment dataset\n",
    "splits = [\"train\", \"test\"]\n",
    "ds = {split: ds for split, ds in zip(splits, load_dataset(\"climatebert/climate_sentiment\", split=splits))}\n",
    "\n",
    "# Show the dataset\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67c907c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '− Scope 3: Optional scope that includes indirect emissions associated with the goods and services supply chain produced outside the organization. Included are emissions from the transport of products from our logistics centres to stores (downstream) performed by external logistics operators (air, land and sea transport) as well as the emissions associated with electricity consumption in franchise stores.',\n",
       " 'label': 1}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect the first element. For labels, 0 is risk, 1 is neutral, and 2 is opportunity\n",
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b04a05",
   "metadata": {},
   "source": [
    "### Pre-process datasets\n",
    "The dataset needs to be processed by converting all of the text into tokens for the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "019b9f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Assign padding token\n",
    "tokenizer.build_inputs_with_special_tokens(tokenizer.all_special_tokens)  # Rebuild vocabulary\n",
    "\n",
    "# Tokenize dataset\n",
    "tokenized_dataset = {}\n",
    "for split in splits:\n",
    "    tokenized_dataset[split] = ds[split].map(\n",
    "        lambda x: tokenizer(x['text'], padding = 'max_length', truncation=True), batched=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eeb2c3a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 1000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "     num_rows: 320\n",
       " })}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5176b07f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '− Scope 3: Optional scope that includes indirect emissions associated with the goods and services supply chain produced outside the organization. Included are emissions from the transport of products from our logistics centres to stores (downstream) performed by external logistics operators (air, land and sea transport) as well as the emissions associated with electricity consumption in franchise stores.',\n",
       " 'label': 1,\n",
       " 'input_ids': [14095,\n",
       "  41063,\n",
       "  513,\n",
       "  25,\n",
       "  32233,\n",
       "  8354,\n",
       "  326,\n",
       "  3407,\n",
       "  12913,\n",
       "  8971,\n",
       "  3917,\n",
       "  351,\n",
       "  262,\n",
       "  7017,\n",
       "  290,\n",
       "  2594,\n",
       "  5127,\n",
       "  6333,\n",
       "  4635,\n",
       "  2354,\n",
       "  262,\n",
       "  4009,\n",
       "  13,\n",
       "  34774,\n",
       "  389,\n",
       "  8971,\n",
       "  422,\n",
       "  262,\n",
       "  4839,\n",
       "  286,\n",
       "  3186,\n",
       "  422,\n",
       "  674,\n",
       "  26355,\n",
       "  19788,\n",
       "  284,\n",
       "  7000,\n",
       "  357,\n",
       "  2902,\n",
       "  5532,\n",
       "  8,\n",
       "  6157,\n",
       "  416,\n",
       "  7097,\n",
       "  26355,\n",
       "  12879,\n",
       "  357,\n",
       "  958,\n",
       "  11,\n",
       "  1956,\n",
       "  290,\n",
       "  5417,\n",
       "  4839,\n",
       "  8,\n",
       "  355,\n",
       "  880,\n",
       "  355,\n",
       "  262,\n",
       "  8971,\n",
       "  3917,\n",
       "  351,\n",
       "  8744,\n",
       "  7327,\n",
       "  287,\n",
       "  8663,\n",
       "  7000,\n",
       "  13,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  ...],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the first element. The tokenized elements are stored in 'input_ids'\n",
    "tokenized_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb325336",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=3, bias=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import GPT2ForSequenceClassification\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    'gpt2',\n",
    "    num_labels=3,\n",
    "    id2label={0: 'risk', 1: 'neutral', 2: 'opportunity'},\n",
    "    label2id={'risk': 0, 'neutral': 1, 'opportunity': 2}\n",
    ")\n",
    "\n",
    "# Freeze all the parameters of the base model using param.requires_grad = False\n",
    "# more info here: https://huggingface.co/transformers/v4.2.2/training.html\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Use model.score to output the final classification layer for GPT2. In others it may be model.classifier\n",
    "model.score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18b8e897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2ForSequenceClassification(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (score): Linear(in_features=768, out_features=3, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Print full model parameters\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f168898",
   "metadata": {},
   "source": [
    "### Evaluate base model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "09a0bd6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=3, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c104ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label:  neutral\n",
      "Predicted index:  tensor([1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = tokenized_dataset[\"test\"][0][\"input_ids\"]\n",
    "attention_mask = tokenized_dataset[\"test\"][0][\"attention_mask\"]\n",
    "\n",
    "# Convert to tensors (assuming attention_mask is a list)\n",
    "input_ids = torch.tensor([input_ids])\n",
    "attention_mask = torch.tensor([attention_mask])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    predicted_label_index = torch.argmax(logits, dim=-1)\n",
    "    predicted_label = model.config.id2label[predicted_label_index.item()]\n",
    "\n",
    "\n",
    "print('Predicted label: ', predicted_label)\n",
    "print('Predicted index: ', predicted_label_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be79ca26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Verizon’s environmental, health and safety management system provides a framework for identifying, controlling, and reducing the risks associated with the environments in which we operate. Besides regular management system assessments, internal and third-party compliance audits and inspections are performed annually at hundreds of facilities worldwide. The goal of these assessments is to identify and correct site-specific issues, and to educate and empower facility managers and supervisors to implement corrective actions. Verizon’s environment, health and safety efforts are directed and supported by experienced experts around the world that support our operations and facilities.',\n",
       " 'label': 1,\n",
       " 'input_ids': [13414,\n",
       "  8637,\n",
       "  447,\n",
       "  247,\n",
       "  82,\n",
       "  6142,\n",
       "  11,\n",
       "  1535,\n",
       "  290,\n",
       "  3747,\n",
       "  4542,\n",
       "  1080,\n",
       "  3769,\n",
       "  257,\n",
       "  9355,\n",
       "  329,\n",
       "  13720,\n",
       "  11,\n",
       "  12755,\n",
       "  11,\n",
       "  290,\n",
       "  8868,\n",
       "  262,\n",
       "  7476,\n",
       "  3917,\n",
       "  351,\n",
       "  262,\n",
       "  12493,\n",
       "  287,\n",
       "  543,\n",
       "  356,\n",
       "  8076,\n",
       "  13,\n",
       "  16238,\n",
       "  3218,\n",
       "  4542,\n",
       "  1080,\n",
       "  21837,\n",
       "  11,\n",
       "  5387,\n",
       "  290,\n",
       "  2368,\n",
       "  12,\n",
       "  10608,\n",
       "  11846,\n",
       "  41973,\n",
       "  290,\n",
       "  30287,\n",
       "  389,\n",
       "  6157,\n",
       "  13844,\n",
       "  379,\n",
       "  5179,\n",
       "  286,\n",
       "  7291,\n",
       "  8688,\n",
       "  13,\n",
       "  383,\n",
       "  3061,\n",
       "  286,\n",
       "  777,\n",
       "  21837,\n",
       "  318,\n",
       "  284,\n",
       "  5911,\n",
       "  290,\n",
       "  3376,\n",
       "  2524,\n",
       "  12,\n",
       "  11423,\n",
       "  2428,\n",
       "  11,\n",
       "  290,\n",
       "  284,\n",
       "  20771,\n",
       "  290,\n",
       "  17549,\n",
       "  6841,\n",
       "  11663,\n",
       "  290,\n",
       "  36520,\n",
       "  284,\n",
       "  3494,\n",
       "  46534,\n",
       "  4028,\n",
       "  13,\n",
       "  18062,\n",
       "  447,\n",
       "  247,\n",
       "  82,\n",
       "  2858,\n",
       "  11,\n",
       "  1535,\n",
       "  290,\n",
       "  3747,\n",
       "  4040,\n",
       "  389,\n",
       "  7924,\n",
       "  290,\n",
       "  4855,\n",
       "  416,\n",
       "  5924,\n",
       "  6154,\n",
       "  1088,\n",
       "  262,\n",
       "  995,\n",
       "  326,\n",
       "  1104,\n",
       "  674,\n",
       "  4560,\n",
       "  290,\n",
       "  7291,\n",
       "  13,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  50256,\n",
       "  ...],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  ...]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset['test'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11936daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dz/71j5m95x01d2wwbm3pz62vz80000gn/T/ipykernel_16851/969164050.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  metric = load_metric('accuracy')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Predictions:  [1, 1, 1, 1, 1]\n",
      "Test Set Labels:  [0, 1, 1, 0, 0]\n",
      "Test Set Accuracy:  {'accuracy': 0.4}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric('accuracy')\n",
    "\n",
    "def predict(model, tokenized_dataset, split=\"test\"):\n",
    "  \"\"\"\n",
    "  Function to make predictions on a specific split of the tokenized dataset.\n",
    "  \"\"\"\n",
    "\n",
    "  predictions = [] # Collect predictions\n",
    "  labels = [] # Collect true labels for evaluation\n",
    "  for i, datapoint in enumerate(tokenized_dataset[split]):\n",
    "    if i >= 5:\n",
    "      break\n",
    "    input_ids = torch.tensor([datapoint[\"input_ids\"]])\n",
    "    attention_mask = torch.tensor([datapoint[\"attention_mask\"]])\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(input_ids, attention_mask=attention_mask)\n",
    "      logits = outputs.logits\n",
    "      predicted_label_index = torch.argmax(logits, dim=-1)\n",
    "      predicted_label = model.config.id2label[predicted_label_index.item()]\n",
    "      predictions.append(predicted_label_index.item())\n",
    "      labels.append(datapoint['label'])\n",
    "      \n",
    "  # Assuming model outputs integers 0, 1, 2 for labels\n",
    "  # predictions_int = [model.config.label2id[pred] for pred in predictions]\n",
    "  labels_string = [model.config.id2label[label] for label in labels]\n",
    "\n",
    "  # # Use converted integer labels for metric calculation\n",
    "  metric.add_batch(predictions=predictions, references=labels)\n",
    "  accuracy = metric.compute()\n",
    "  # accuracy_alt = compute_metrics(predictions, labels)\n",
    "\n",
    "  return predictions, labels, accuracy#, accuracy_alt\n",
    "\n",
    "# Make predictions on the test set and calculate accuracy\n",
    "base_predictions, base_labels, base_accuracy = predict(model, tokenized_dataset, split=\"test\")\n",
    "print(\"Test Set Predictions: \", base_predictions)\n",
    "print(\"Test Set Labels: \", base_labels)\n",
    "print(\"Test Set Accuracy: \", base_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ef316e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a2929316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "659ffd25",
   "metadata": {},
   "source": [
    "### Train model without Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25f9662e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewwrist/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      " 50%|█████     | 500/1000 [03:52<03:57,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.0265, 'grad_norm': 0.27658286690711975, 'learning_rate': 0.001, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [07:45<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.9409, 'grad_norm': 14.449419975280762, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 1000/1000 [09:43<00:00,  2.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.417924165725708, 'eval_accuracy': 0.509375, 'eval_runtime': 118.4536, 'eval_samples_per_second': 2.701, 'eval_steps_per_second': 2.701, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [09:44<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 584.3157, 'train_samples_per_second': 1.711, 'train_steps_per_second': 1.711, 'train_loss': 1.9837210083007812, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=1.9837210083007812, metrics={'train_runtime': 584.3157, 'train_samples_per_second': 1.711, 'train_steps_per_second': 1.711, 'train_loss': 1.9837210083007812, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy': (predictions == labels).mean()}\n",
    "\n",
    "# Use HuggingFace Trainer class for training and evaluating mbase model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/sentiment_analysis\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=1, # Keeping low for low memory\n",
    "        per_device_eval_batch_size=1,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dabb422d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [01:50<00:00,  2.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.3843942880630493,\n",
       " 'eval_accuracy': 0.509375,\n",
       " 'eval_runtime': 111.0946,\n",
       " 'eval_samples_per_second': 2.88,\n",
       " 'eval_steps_per_second': 2.88,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d52a229",
   "metadata": {},
   "source": [
    "## Performing Parameter-Efficient Fine-Tuning\n",
    "\n",
    "TODO: In the cells below, create a PEFT model from your loaded model, run a training loop, and save the PEFT model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf071cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"mps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5775fadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "config = LoraConfig(\n",
    "    r=2,  # Rank of LoRA decomposition; low rank to start out quickly training\n",
    "    lora_alpha=16,  # Learning rate for LoRA weights\n",
    "    lora_dropout=0.05,  # Dropout rate for LoRA weights\n",
    "    bias=\"none\",  # Remove biases from the original model\n",
    "    task_type=\"CLASSIFICATION\"  # Text classification task\n",
    "    ) #Look at LoRA adapter documentation for additional hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894046c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM\n",
    "# model = AutoModelForCausalLM.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4d4c908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewwrist/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/peft/tuners/lora/layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from peft import get_peft_model\n",
    "lora_model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b47abf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 73,728 || all params: 124,515,840 || trainable%: 0.059211743662493065\n"
     ]
    }
   ],
   "source": [
    "lora_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d7264b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): GPT2ForSequenceClassification(\n",
       "      (transformer): GPT2Model(\n",
       "        (wte): Embedding(50257, 768)\n",
       "        (wpe): Embedding(1024, 768)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (h): ModuleList(\n",
       "          (0-11): 12 x GPT2Block(\n",
       "            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): GPT2Attention(\n",
       "              (c_attn): lora.Linear(\n",
       "                (base_layer): Conv1D()\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=768, out_features=2, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=2, out_features=2304, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "              )\n",
       "              (c_proj): Conv1D()\n",
       "              (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "              (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): GPT2MLP(\n",
       "              (c_fc): Conv1D()\n",
       "              (c_proj): Conv1D()\n",
       "              (act): NewGELUActivation()\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (score): Linear(in_features=768, out_features=3, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "39757c44",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/andrewwrist/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/codecarbon/data/hardware/cpu_power.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcodecarbon\u001b[39;00m \u001b[39mimport\u001b[39;00m EmissionsTracker\n\u001b[1;32m      3\u001b[0m \u001b[39m# Initialize carbon tracker to track energy consumption\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m tracker \u001b[39m=\u001b[39m EmissionsTracker(\n\u001b[1;32m      5\u001b[0m     project_name\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlora_climate_train\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      6\u001b[0m     output_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./data/carbon_tracking\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     output_file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39memissions.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     log_level\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39merror\u001b[39;49m\u001b[39m'\u001b[39;49m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m tracker\u001b[39m.\u001b[39mstart()\n\u001b[1;32m     11\u001b[0m \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m tracker\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[0;32m~/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/codecarbon/emissions_tracker.py:327\u001b[0m, in \u001b[0;36mBaseEmissionsTracker.__init__\u001b[0;34m(self, project_name, measure_power_secs, api_call_interval, api_endpoint, api_key, output_dir, output_file, save_to_file, save_to_api, save_to_logger, logging_logger, save_to_prometheus, prometheus_url, gpu_ids, emissions_endpoint, experiment_id, experiment_name, co2_signal_api_token, tracking_mode, log_level, on_csv_write, logger_preamble, default_cpu_power, pue)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    324\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[1;32m    325\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo CPU tracking mode found. Falling back on CPU constant mode.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    326\u001b[0m     )\n\u001b[0;32m--> 327\u001b[0m     tdp \u001b[39m=\u001b[39m cpu\u001b[39m.\u001b[39;49mTDP()\n\u001b[1;32m    328\u001b[0m     power \u001b[39m=\u001b[39m tdp\u001b[39m.\u001b[39mtdp\n\u001b[1;32m    329\u001b[0m     model \u001b[39m=\u001b[39m tdp\u001b[39m.\u001b[39mmodel\n",
      "File \u001b[0;32m~/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/codecarbon/core/cpu.py:283\u001b[0m, in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/codecarbon/core/cpu.py:369\u001b[0m, in \u001b[0;36m_main\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/codecarbon/core/cpu.py:291\u001b[0m, in \u001b[0;36m_get_cpu_power_from_registry\u001b[0;34m(self, cpu_model_raw)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/codecarbon/input.py:121\u001b[0m, in \u001b[0;36mget_cpu_power_data\u001b[0;34m(self)\u001b[0m\n",
      "File \u001b[0;32m~/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    613\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m~/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[1;32m   1706\u001b[0m     f,\n\u001b[1;32m   1707\u001b[0m     mode,\n\u001b[1;32m   1708\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1709\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1710\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1711\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1712\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1713\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1714\u001b[0m )\n\u001b[1;32m   1715\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    864\u001b[0m             handle,\n\u001b[1;32m    865\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[1;32m    866\u001b[0m             encoding\u001b[39m=\u001b[39mioargs\u001b[39m.\u001b[39mencoding,\n\u001b[1;32m    867\u001b[0m             errors\u001b[39m=\u001b[39merrors,\n\u001b[1;32m    868\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    869\u001b[0m         )\n\u001b[1;32m    870\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/andrewwrist/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/codecarbon/data/hardware/cpu_power.csv'"
     ]
    }
   ],
   "source": [
    "# from codecarbon import EmissionsTracker\n",
    "\n",
    "# # Initialize carbon tracker to track energy consumption\n",
    "# tracker = EmissionsTracker(\n",
    "#     project_name='lora_climate_train',\n",
    "#     output_dir='./data/carbon_tracking',\n",
    "#     output_file='emissions.csv',\n",
    "#     log_level='error'\n",
    "# )\n",
    "# tracker.start()\n",
    "# float = tracker.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "574cb045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewwrist/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      " 50%|█████     | 500/1000 [08:06<08:01,  1.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4601, 'grad_norm': 0.10177532583475113, 'learning_rate': 0.001, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [16:27<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1495, 'grad_norm': 2.6937546730041504, 'learning_rate': 0.0, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 1000/1000 [18:51<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 144.1881, 'eval_samples_per_second': 2.219, 'eval_steps_per_second': 2.219, 'epoch': 1.0}\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'eval_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 30\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Use HuggingFace Trainer class for training and evaluating mbase model\u001b[39;00m\n\u001b[1;32m     10\u001b[0m lora_trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     11\u001b[0m     model\u001b[39m=\u001b[39mlora_model,\n\u001b[1;32m     12\u001b[0m     args\u001b[39m=\u001b[39mTrainingArguments(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     compute_metrics\u001b[39m=\u001b[39mcompute_metrics,\n\u001b[1;32m     28\u001b[0m     )\n\u001b[0;32m---> 30\u001b[0m lora_trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[0;32m~/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/transformers/trainer.py:1780\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1778\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1780\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1781\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m   1782\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[1;32m   1783\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[1;32m   1784\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[1;32m   1785\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/transformers/trainer.py:2213\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2210\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_training_stop \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   2212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_epoch_end(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m-> 2213\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_log_save_evaluate(tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\n\u001b[1;32m   2215\u001b[0m \u001b[39mif\u001b[39;00m DebugOption\u001b[39m.\u001b[39mTPU_METRICS_DEBUG \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdebug:\n\u001b[1;32m   2216\u001b[0m     \u001b[39mif\u001b[39;00m is_torch_xla_available():\n\u001b[1;32m   2217\u001b[0m         \u001b[39m# tpu-comment: Logging debug metrics for PyTorch/XLA (compile, execute times, ops, etc.)\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/transformers/trainer.py:2588\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2585\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlr_scheduler\u001b[39m.\u001b[39mstep(metrics[metric_to_check])\n\u001b[1;32m   2587\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol\u001b[39m.\u001b[39mshould_save:\n\u001b[0;32m-> 2588\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_checkpoint(model, trial, metrics\u001b[39m=\u001b[39;49mmetrics)\n\u001b[1;32m   2589\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_save(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n",
      "File \u001b[0;32m~/Documents/Projects/AI_ML/venv/lib/python3.11/site-packages/transformers/trainer.py:2669\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[0;34m(self, model, trial, metrics)\u001b[0m\n\u001b[1;32m   2667\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m metric_to_check\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39meval_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m   2668\u001b[0m     metric_to_check \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39meval_\u001b[39m\u001b[39m{\u001b[39;00mmetric_to_check\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 2669\u001b[0m metric_value \u001b[39m=\u001b[39m metrics[metric_to_check]\n\u001b[1;32m   2671\u001b[0m operator \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mgreater \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mgreater_is_better \u001b[39melse\u001b[39;00m np\u001b[39m.\u001b[39mless\n\u001b[1;32m   2672\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   2673\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mbest_metric \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2674\u001b[0m     \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mbest_model_checkpoint \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   2675\u001b[0m     \u001b[39mor\u001b[39;00m operator(metric_value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mbest_metric)\n\u001b[1;32m   2676\u001b[0m ):\n",
      "\u001b[0;31mKeyError\u001b[0m: 'eval_loss'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import DataCollatorWithPadding, Trainer, TrainingArguments\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'accuracy': (predictions == labels).mean()}\n",
    "\n",
    "# Use HuggingFace Trainer class for training and evaluating mbase model\n",
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=TrainingArguments(\n",
    "        output_dir=\"./data/lora\",\n",
    "        learning_rate=2e-3,\n",
    "        per_device_train_batch_size=1, # Keeping low for low memory\n",
    "        per_device_eval_batch_size=1,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "    ),\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['test'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "lora_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c8b23fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \n",
      "100%|██████████| 1000/1000 [24:51<00:00,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_runtime': 134.6331, 'eval_samples_per_second': 2.377, 'eval_steps_per_second': 2.377, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_runtime': 134.6331,\n",
       " 'eval_samples_per_second': 2.377,\n",
       " 'eval_steps_per_second': 2.377,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa7fe003",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model.save_pretrained('gpt_lora_climate_sentiment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615b12c6",
   "metadata": {},
   "source": [
    "## Performing Inference with a PEFT Model\n",
    "\n",
    "TODO: In the cells below, load the saved PEFT model weights and evaluate the performance of the trained PEFT model. Be sure to compare the results to the results from prior to fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863ec66e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "lora_model = AutoPeftModelForCausalLM.from_pretrained('gpt_lora_climate_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a8147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer_lora = AutoTokenizer.from_pretrained('gpt2')\n",
    "inputs = tokenizer_lora(...<mask>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc96905a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866ab28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a32e4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "6b5b525088d6332c5cf70b03a9f5e180ca0f3c63042aef5ed677c6c58b7b131a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
